{
 "metadata": {
  "name": "",
  "signature": "sha256:141aa893ff5a18eb515e79653b4175a3c43410351f5071e94cd8c4cef2774612"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Representing the Logistic Regression Hypothesis\n",
      "\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Ok, we want $ 0 \\leq h_\\theta(x) \\leq 1 $\n",
      "            \n",
      "Remember this representation:           \n",
      "$ h_\\theta = g(\\theta^Tx) $      \n",
      "          \n",
      "where:      \n",
      "$ g(z) = \\frac{1}{1 + e^{-z}} $       \n",
      "       \n",
      "So our final representation is:    \n",
      "$ g(z) = \\frac{1}{1 + e^{-\\theta^Tx}} $\n",
      "\n",
      "__this is the *sigmoid* or *logistic* function__"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# define the logistic function\n",
      "\n",
      "import numpy as np\n",
      "def sigmoid(x, theta=np.array(1.0)):\n",
      "    x = np.array(x)\n",
      "    theta = np.array(theta)\n",
      "    return(1 / (1 + np.exp(-x.dot(theta.T))))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 17
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# cool, let's visualize it (x is 'z', y is 'g(z)')\n",
      "import matplotlib.pyplot as plt\n",
      "from pylab import *\n",
      "\n",
      "# notebook magic\n",
      "%matplotlib inline\n",
      "\n",
      "# map the sigmoid function over a vector of numbers\n",
      "x = np.arange(-6, 6, .01)\n",
      "y = [ sigmoid(n) for n in x ]\n",
      "\n",
      "# add a vertical line at 0\n",
      "plt.axvline(x=0, ymin=0, ymax=1, ls='--')\n",
      "\n",
      "plt.plot(x, y, color='red', lw=2)\n",
      "plt.xlabel('z')\n",
      "plt.ylabel('g(z)')\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "display_data",
       "png": "iVBORw0KGgoAAAANSUhEUgAAAYEAAAEPCAYAAACk43iMAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYII=\n",
       "text": [
        "<matplotlib.figure.Figure at 0x1d6a910>"
       ]
      }
     ],
     "prompt_number": 18
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Implementing Logistic Regression\n",
      "__our goal is to fit the parameters $ \\theta $ to our data__\n",
      "\n",
      "\n",
      "### Interpreting the hypothesis\n",
      "__The key idea is to interpret $ h_\\theta(x) $ as the probability that y = 1 for input x__     \n",
      "In other words, if x >= 0.5 we classify it as 1, if it's < 0.5, we classify it as 0      \n",
      "\n",
      "formally:     \n",
      "$ h_\\theta(x) = p(y=1 | x;\\theta) $\n",
      ">__\"the probability that y=1, given x, parameterized by $ \\theta $\"__ - Andrew Ng        \n",
      "      \n",
      "Note the following as well:     \n",
      "$ p(y=0| x;\\theta) + p(y=1| x;\\theta) = 1 $           \n",
      "$ p(y=0| x;\\theta) = 1 - p(y=1| x;\\theta) $    \n",
      "        \n",
      "Question: what values of z cause g(z) to be > 0.5 ?\n",
      "\n",
      "Note:     \n",
      "$ h_\\theta(x) = g(\\theta^Tx) \\geq 0.5 $ whenever $ \\theta^Tx \\geq 0 $       \n",
      "and conversely:      \n",
      "$ h_\\theta(x) = g(\\theta^Tx) \\lt 0.5 $ whenever $ \\theta^Tx \\lt 0 $"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### understanding and visualizing the decision boundary\n",
      "Remember that: $ h_\\theta(x) = g(\\theta_0 + \\theta_1x_1 + \\theta_2x_2) $      \n",
      "\n",
      "__possible exercise__:\n",
      "visualize the classes of points for a random array of sample data, and a sample parameter vector     \n",
      "if params are $ \\theta = [ -3,1,1 ] $, then the line for the decision boundary is $ -x + 3 $     "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# notebook magic\n",
      "%matplotlib inline\n",
      "from numpy.random import random, randint\n",
      "\n",
      "def boundary(x):\n",
      "    x = np.float(x)\n",
      "    return (-x + 3)\n",
      "\n",
      "def random_point():\n",
      "    return (1, random()*randint(0,6), random()*randint(0,6))\n",
      "\n",
      "def decide(value):\n",
      "    if value >= 0.5:\n",
      "        return 1\n",
      "    return 0\n",
      "\n",
      "\n",
      "# an example vector of theta values\n",
      "example_theta = np.array([-3,1,1])\n",
      "\n",
      "plt.clf()\n",
      "markers = ['o', '+']\n",
      "colors = ['red', 'blue']\n",
      "points_x = [ random_point() for x in range(100) ]\n",
      "classes = [ decide(sigmoid(p, example_theta)) for p in points_x ]\n",
      "x_1 = [x[1] for x in points_x]\n",
      "x_2 = [x[2] for x in points_x]\n",
      "for i,p in enumerate(points_x):\n",
      "    x_1 = p[1]\n",
      "    x_2 = p[2]\n",
      "    c=classes[i]\n",
      "    plt.scatter(x_1, x_2, s=30.0, c=c, marker=markers[c], color=colors[c])\n",
      "    \n",
      "\n",
      "# map the sigmoid function over a vector of numbers\n",
      "x = np.arange(0, 3, .01)\n",
      "y = [ boundary(n) for n in x ]\n",
      "\n",
      "# add a vertical line at 0\n",
      "#plt.axvline(x=0, ymin=0, ymax=1, ls='--')\n",
      "\n",
      "plt.plot(x, y, color='red', lw=1, ls='--')\n",
      "plt.xlabel('x_1')\n",
      "plt.ylabel('x_2')\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "display_data",
       "png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAEQCAYAAAC5oaP8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nJAEIIYSH+n9dBfhfPDGf+gAAAABJRU5ErkJggg==\n",
       "text": [
        "<matplotlib.figure.Figure at 0x4378b50>"
       ]
      }
     ],
     "prompt_number": 19
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Non-Linear decision boundaries\n",
      "__in logistic regression, to capture a non-linear boundary, we need to add polynomial features__     \n",
      "i.e.    \n",
      "$ h_\\theta(x) = g(\\theta_0 + \\theta_1x_1 + \\theta_2x_2 + \\theta_3x^2_1 + \\theta_4x^2_2) $       \n",
      "       \n",
      "Question: what happens if the $ \\theta $ values for the features above are [-1,0,0,1,1]?      \n",
      "Answer: predict y=1 when $ -1 + x^2_1 + x^2_2 \\geq 0 $       \n",
      "So the decision boundary is: $ x^2_1 + x^2_2 = 1 $ (a circle)               \n",
      "__Remember that the decision boundary is defined by $ \\theta $, not by the dataset__           \n",
      "--> higher-order polynomial features can make arbitrarily complex decision boundaries"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Cost Function -- aka Optimization Objective        \n",
      "We have a training set: $ \\{(x^{(1)}, y^{(1)}), (x^{(2)}, y^{(2)}), ...,(x^{(m)}, y^{(m)}) \\} $          \n",
      "__each $x$ is a vector: $ [x_0, x_1, x_2, ..., x_n] $__         \n",
      "* note that $X_0 = 1$ ($\\theta_0$ is the 'bias' term)        \n",
      "since this is a classification problem, $ y \\in \\{0,1\\} $          \n",
      "       \n",
      "and remember:       \n",
      "$ g(z) = \\frac{1}{1 + e^{-\\theta^Tx}} $          \n",
      "        \n",
      "__So how do we find the right $\\theta$s?__     \n",
      "          \n",
      "For linear regression, the cost function was:         \n",
      "$J(\\theta) = \\frac{1}{m}\\sum\\limits_{i=1}^m\\frac{1}{2}(h_\\theta(x^{(i)}) - y^{(i)})^2 $\n",
      "       \n",
      "Let's represent the part after the sum as:     \n",
      "$ cost(h_\\theta(x^{(i)}, y^{(i)}) $           \n",
      "       \n",
      "So, for linear regression:     \n",
      "$ cost(h_\\theta(x^{(i)}, y^{(i)}) = \\frac{1}{2}(h_\\theta(x^{(i)}) - y^{(i)})^2 $          \n",
      "if we just define $h_\\theta(x)$ as the logistic (sigmoid) function, it's not convex    \n",
      "      \n",
      "So, redefine the cost function as:       \n",
      "          \n",
      "$ cost(h_\\theta(x), y) = \n",
      "\\begin{cases}\n",
      "-log(h_\\theta(x))~~if~~y = 1 \\\\\n",
      "-log(1 - h_\\theta(x))~~if~~y = 0\n",
      "\\end{cases} $                 \n",
      "           \n",
      "      \n",
      "### Get ready, something cool is coming....     \n",
      "because $ y = \\{0, 1\\} $      \n",
      "you can write the cost function succinctly as:      \n",
      "       \n",
      "$ cost(h_\\theta(x^{(i)}, y^{(i)}) = -y~log(h_\\theta(x)) - (1-y)log(1 - h_\\theta(x)) $    \n",
      "      \n",
      "__Important: convince yourself that this is true__       \n",
      "       \n",
      "The final version of the LR cost function for the whole dataset:      \n",
      "$ J(\\theta) = -\\frac{1}{m}[\\sum\\limits_{i=1}^m~y^{(i)}~log(h_\\theta(x^{(i)})) - (1-y^{(i)})log(1 - h_\\theta(x^{(i)}))] $\n",
      "                  "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Training\n",
      "__We want to find:__\n",
      "       \n",
      "$\\displaystyle \\min_{\\theta}J(\\theta) $\n",
      "       \n",
      "given parameters $x$, we predict the $y$ value by:     \n",
      "$ h_\\theta(x) = \\frac{1}{1 + e^{-\\theta^Tx}} $     \n",
      "aka:    \n",
      "$ p(y=1 | x;\\theta) $            \n",
      "       \n",
      "The update template is:      \n",
      "Repeat {       \n",
      "$~~~~\\theta_j := \\theta_j - \\alpha\\frac{\\partial}{\\partial\\theta_j}J(\\theta) $            \n",
      "}     \n",
      "$\\alpha$ is the learning rate     \n",
      "We update all $\\theta_j$ __simultaeneously__     \n",
      "       \n",
      "__Note: don't fear the partial derivative, it's really simple in practice:__     \n",
      "      \n",
      "$ \\frac{\\partial}{\\partial\\theta_j}J(\\theta) = \\frac{1}{m}\\sum\\limits_{i=1}^m(h_\\theta(x^{(i)}) - y^{(i)})x_j^{(i)} $             \n",
      "       \n",
      "__so the update equation becomes: __      \n",
      "\n",
      "Repeat {       \n",
      "$~~~~\\theta_j := \\theta_j - \\alpha\\frac{1}{m}\\sum\\limits_{i=1}^m(h_\\theta(x^{(i)}) - y^{(i)})x_j^{(i)} $            \n",
      "}      \n",
      "__this is identical to the update rule for gradient descent for Linear Regression__      \n",
      "       \n",
      "the vectorized version of the update template (note the missing $j$s):      \n",
      "\n",
      "$~~~~\\theta := \\theta - \\alpha\\frac{1}{m}\\sum\\limits_{i=1}^m[(h_\\theta(x^{(i)}) - y^{(i)})x^{(i)}] $      "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Prepare a Dataset\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# some binary classification datasets can be found here: http://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary.html\n",
      "from sklearn.datasets import load_iris\n",
      "import random\n",
      "\n",
      "# Get some data and munge it into the format we want\n",
      "data = load_iris()\n",
      "binary_iris = np.array(data.target) == 0\n",
      "\n",
      "class_map = { True: 1.0, False: 0.0 }\n",
      "binary_y = np.array([ class_map[c] for c in binary_iris ])\n",
      "\n",
      "# the full datasets\n",
      "X = data.data[:data.data.shape[0] - np.int(data.data.shape[0] / 3), :]\n",
      "y = binary_y[:data.data.shape[0] - np.int(data.data.shape[0] / 3)]\n",
      "\n",
      "# split test/train\n",
      "id_list = range(len(data.target))\n",
      "random.shuffle(id_list)\n",
      "train_X = data.data[[id_list[:120]]]\n",
      "test_X = data.data[[id_list[120:]]]\n",
      "train_y = binary_y[[id_list[:120]]]\n",
      "test_y = binary_y[[id_list[120:]]]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 20
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pandas as pd\n",
      "# scikit-learn dimension reduction\n",
      "from sklearn.decomposition import PCA\n",
      "\n",
      "# scikit-learn dataset processing utils\n",
      "from sklearn.preprocessing import MinMaxScaler\n",
      "\n",
      "df = pd.read_csv('./data/train.csv')\n",
      "df = df.astype('float64')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 21
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 22,
       "text": [
        "(42000, 785)"
       ]
      }
     ],
     "prompt_number": 22
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "min_max_scaler = MinMaxScaler()\n",
      "pca = PCA(n_components=80)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 23
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X = min_max_scaler.fit_transform(df.ix[:9999,1:])\n",
      "train_X = pca.fit_transform(X)\n",
      "\n",
      "train_y = df.ix[:9999,0]\n",
      "\n",
      "# Xt = min_max_scaler.transform(df.ix[35000:,1:])\n",
      "test_X = min_max_scaler.transform(df.ix[35000:,1:])\n",
      "test_X = pca.transform(test_X)\n",
      "# yt = df.ix[35000:,0]\n",
      "test_y = df.ix[35000:,0] \n",
      "train_y[:10]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 33,
       "text": [
        "0    1\n",
        "1    0\n",
        "2    1\n",
        "3    4\n",
        "4    0\n",
        "5    0\n",
        "6    7\n",
        "7    3\n",
        "8    5\n",
        "9    3\n",
        "Name: label, dtype: float64"
       ]
      }
     ],
     "prompt_number": 33
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class_map = { True: 1.0, False: 0.0 }\n",
      "\n",
      "# binarize train_y and test_y\n",
      "train_y = np.array(train_y) == 1\n",
      "test_y = np.array(test_y) == 1\n",
      "\n",
      "train_y = np.array([ class_map[c] for c in train_y ])\n",
      "test_y = np.array([ class_map[c] for c in test_y ])\n",
      "test_y[:10]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 35,
       "text": [
        "array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.])"
       ]
      }
     ],
     "prompt_number": 35
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "train_y[:10]\n",
      "train_y = np.array(train_y) == 1\n",
      "train_y[:10]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 36,
       "text": [
        "array([ True, False,  True, False, False, False, False, False, False, False], dtype=bool)"
       ]
      }
     ],
     "prompt_number": 36
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Implement the update equation"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# compute the hypothesis for the row, subtract y, and put the value in every cell of that row\n",
      "# scale each cell by the x_j of that cell (multiply cellwise with the original array)\n",
      "# average columns, and multiply by alpha\n",
      "# subtract that vector from the previous theta to get the new theta for this iteration\n",
      "\n",
      "# this theta will get updated as the algorithm iterates\n",
      "# theta = np.random.uniform(-1.0,1.0,[1,X.shape[1]])\n",
      "theta = np.zeros((1,train_X.shape[1]))\n",
      "original_theta = np.array(theta)\n",
      "lambd = 1.0\n",
      "\n",
      "# completely vectorized implementation\n",
      "# NOTE: not regularized -- do this as an exercise\n",
      "def iterate(X, y, theta, alpha):\n",
      "    updates = np.average(X * (sigmoid(X, theta).T - y).T, axis=0) * alpha \n",
      "    new_theta = theta - updates\n",
      "    return new_theta\n",
      "\n",
      "# just for demo purposes - a more straightforward version of the function above\n",
      "# regularized iterate\n",
      "def regularized_simple_iterate(X, y, theta, alpha):\n",
      "    hypotheses = sigmoid(X, theta)\n",
      "    minus_y = hypotheses.T - y\n",
      "    scaled_by_x = X * minus_y.T\n",
      "    col_averages = np.average(scaled_by_x, axis=0)\n",
      "    updates = col_averages* alpha\n",
      "    \n",
      "    updates2 = col_averages*alpha -lambd*theta[0]*alpha\n",
      "    updates2[0] = updates2[0]+ (lambd*theta[0][0]*alpha)\n",
      "    new_theta = theta - updates\n",
      "    regularized_theta = theta - updates2\n",
      "   \n",
      "    return regularized_theta\n",
      "\n",
      "def simple_iterate(X, y, theta, alpha):\n",
      "    hypotheses = sigmoid(X, theta)\n",
      "    minus_y = hypotheses.T - y\n",
      "    scaled_by_x = X * minus_y.T\n",
      "    col_averages = np.average(scaled_by_x, axis=0)\n",
      "    updates = col_averages* alpha\n",
      "    new_theta = theta - updates\n",
      "    return new_theta"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 37
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Evaluate performance"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# let's test the accuracy of our classifier\n",
      "def predict(x, theta):\n",
      "    return sigmoid(x, theta)\n",
      "\n",
      "def classes(y):\n",
      "    if y == 1.0:\n",
      "        return True\n",
      "    return False\n",
      "\n",
      "def check_accuracy(predictions, y):\n",
      "    binarized_predictions = predictions >= 0.5\n",
      "    tf = np.array([ classes(c) for c in y ], dtype='bool')\n",
      "    correct = binarized_predictions[:,0] == tf\n",
      "    score = np.sum([ 1 for s in correct if s == True ]) / np.float(len(correct))\n",
      "    return(score)\n",
      "  "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 38
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# To predict, all we need is a theta - let's check performance before training\n",
      "check_accuracy(predict(test_X, original_theta), test_y)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 39,
       "text": [
        "0.11157142857142857"
       ]
      }
     ],
     "prompt_number": 39
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Regularized Cost Function"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# note: this is just for sanity checks\n",
      "def cost_function(X, answers, theta):\n",
      "    total = 0.0\n",
      "    for x,y in zip(X,answers):\n",
      "        hyp = sigmoid(x, theta)\n",
      "        total += ( (-y * np.log(hyp)) - ((1-y) * np.log(1-hyp)) ) \n",
      "    cost = (total / len(answers))\n",
      "    return cost"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 40
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# note: this is just for sanity checks\n",
      "def regularized_cost_function(X, answers, theta):\n",
      "    total = 0.0\n",
      "    for x,y in zip(X,answers):\n",
      "        hyp = sigmoid(x, theta)\n",
      "        total += ( (-y * np.log(hyp)) - ((1-y) * np.log(1-hyp)) ) \n",
      "    cost = (total / len(answers)) + ((lambd/len(answers))*sum(theta[1:]*theta[1:]))\n",
      "    return cost"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 41
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# print the original theta, and the output of the cost function\n",
      "print(theta)\n",
      "print(cost_function(train_X, train_y, theta))\n",
      "test =  []\n",
      "\n",
      "def gradient_descent(X, y, theta, alpha, num_iterations=100):\n",
      "    working_theta = np.array(theta)\n",
      "    all_cost =[]\n",
      "    for i in range(num_iterations):\n",
      "        working_theta = regularized_simple_iterate(X, y, working_theta, alpha)\n",
      "#         working_theta = simple_iterate(X, y, working_theta, alpha)\n",
      "        all_cost.append(regularized_cost_function(train_X, train_y, working_theta))\n",
      "#         all_cost.append(cost_function(train_X, train_y, working_theta))\n",
      "        #j = j+1\n",
      "    test.append(all_cost)    \n",
      "    return np.array(working_theta)\n",
      "        \n",
      "num_iterations = 10\n",
      "j = 0   \n",
      "\n",
      "alpha = 0.005\n",
      "\n",
      "best_theta = gradient_descent(train_X, train_y, theta, alpha, num_iterations)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
        "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
        "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
        "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
        "   0.  0.  0.  0.  0.  0.  0.  0.]]\n",
        "[ 0.]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 44
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "predictions = predict(test_X, best_theta)    \n",
      "check_accuracy(predictions, test_y)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 45,
       "text": [
        "0.66357142857142859"
       ]
      }
     ],
     "prompt_number": 45
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# test with some different learning rates\n",
      "alpha = [0.005,0.01,0.03,0.06,0.1]\n",
      "for i in alpha:\n",
      "    best_theta = gradient_descent(train_X, train_y, theta, i, num_iterations)\n",
      "\n",
      "    j=j+1\n",
      "    print i;\n",
      "all_cost\n",
      "\n",
      "plt.clf()\n",
      "plt.plot(test[0],color='yellow',lw=2)\n",
      "plt.plot(test[1],color='blue', lw=2)\n",
      "plt.plot(test[2],color='green', lw=2)\n",
      "plt.plot(test[3],color='red', lw=2)\n",
      "plt.plot(test[4],color='grey', lw=2)\n",
      "plt.xlabel('hypothesis')\n",
      "plt.ylabel('cost')\n",
      "plt.ylim([0,0.2])\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# the following graph shows what the cost function looks like for y = 1 and y = 0\n",
      "plt.clf()\n",
      "\n",
      "def positive_cost(hyp_value):\n",
      "    return -(np.log(hyp_value))\n",
      "\n",
      "def negative_cost(hyp_value):\n",
      "    return -(np.log(1 - hyp_value))\n",
      "\n",
      "# map cost function over some numbers\n",
      "pos1_x = np.arange(0.01, 1, .01)\n",
      "pos1_y = [ positive_cost(n) for n in pos1_x ]\n",
      "\n",
      "\n",
      "neg1_x = np.arange(0.01, 1, .01)\n",
      "neg1_y = [ negative_cost(n) for n in neg1_x ]\n",
      "\n",
      "# add a horizontal line at 0\n",
      "#plt.axvline(x=0, ymin=0, ymax=1, ls='--')\n",
      "\n",
      "plt.plot(pos1_x, pos1_y, color='blue', lw=2)\n",
      "plt.plot(neg1_x, neg1_y, color='red', lw=2)\n",
      "\n",
      "plt.xlabel('hypothesis')\n",
      "plt.ylabel('cost')\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "__Question: Explain why this is a good cost function for learning the parameters of a binary classifier__"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Exercises:      \n",
      "(1) Plot the regularized cost function with several different values of lambda      \n",
      "(2) Increase the number of features (increase the number of principle components in PCA), and plot how the performance changes     \n",
      "(3) Refactor the vectorized 'iterate' function to use regularization\n",
      "(4) Extend our LR implementation to a multi-class implementation"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 12
    }
   ],
   "metadata": {}
  }
 ]
}