{
 "metadata": {
  "name": "",
  "signature": "sha256:141aa893ff5a18eb515e79653b4175a3c43410351f5071e94cd8c4cef2774612"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Representing the Logistic Regression Hypothesis\n",
      "\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Ok, we want $ 0 \\leq h_\\theta(x) \\leq 1 $\n",
      "            \n",
      "Remember this representation:           \n",
      "$ h_\\theta = g(\\theta^Tx) $      \n",
      "          \n",
      "where:      \n",
      "$ g(z) = \\frac{1}{1 + e^{-z}} $       \n",
      "       \n",
      "So our final representation is:    \n",
      "$ g(z) = \\frac{1}{1 + e^{-\\theta^Tx}} $\n",
      "\n",
      "__this is the *sigmoid* or *logistic* function__"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# define the logistic function\n",
      "import numpy as np\n",
      "def sigmoid(x, theta=np.array(1.0)):\n",
      "    x = np.array(x)\n",
      "    theta = np.array(theta)\n",
      "    return(1 / (1 + np.exp(-x.dot(theta.T))))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 13
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# cool, let's visualize it (x is 'z', y is 'g(z)')\n",
      "import matplotlib.pyplot as plt\n",
      "from pylab import *\n",
      "\n",
      "# notebook magic\n",
      "%matplotlib inline\n",
      "\n",
      "# map the sigmoid function over a vector of numbers\n",
      "x = np.arange(-6, 6, .01)\n",
      "y = [ sigmoid(n) for n in x ]\n",
      "\n",
      "# add a vertical line at 0\n",
      "plt.axvline(x=0, ymin=0, ymax=1, ls='--')\n",
      "\n",
      "plt.plot(x, y, color='red', lw=2)\n",
      "plt.xlabel('z')\n",
      "plt.ylabel('g(z)')\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "display_data",
       "png": "iVBORw0KGgoAAAANSUhEUgAAAYEAAAEPCAYAAACk43iMAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSVn/Hz7n2i4zut7PAAAAAElFTkSuQmCC\n",
       "text": [
        "<matplotlib.figure.Figure at 0xa80146c>"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Implementing Logistic Regression\n",
      "__our goal is to fit the parameters $ \\theta $ to our data__\n",
      "\n",
      "\n",
      "### Interpreting the hypothesis\n",
      "__The key idea is to interpret $ h_\\theta(x) $ as the probability that y = 1 for input x__     \n",
      "In other words, if x >= 0.5 we classify it as 1, if it's < 0.5, we classify it as 0      \n",
      "\n",
      "formally:     \n",
      "$ h_\\theta(x) = p(y=1 | x;\\theta) $\n",
      ">__\"the probability that y=1, given x, parameterized by $ \\theta $\"__ - Andrew Ng        \n",
      "      \n",
      "Note the following as well:     \n",
      "$ p(y=0| x;\\theta) + p(y=1| x;\\theta) = 1 $           \n",
      "$ p(y=0| x;\\theta) = 1 - p(y=1| x;\\theta) $    \n",
      "        \n",
      "Question: what values of z cause g(z) to be > 0.5 ?\n",
      "\n",
      "Note:     \n",
      "$ h_\\theta(x) = g(\\theta^Tx) \\geq 0.5 $ whenever $ \\theta^Tx \\geq 0 $       \n",
      "and conversely:      \n",
      "$ h_\\theta(x) = g(\\theta^Tx) \\lt 0.5 $ whenever $ \\theta^Tx \\lt 0 $"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### understanding and visualizing the decision boundary\n",
      "Remember that: $ h_\\theta(x) = g(\\theta_0 + \\theta_1x_1 + \\theta_2x_2) $      \n",
      "\n",
      "__possible exercise__:\n",
      "visualize the classes of points for a random array of sample data, and a sample parameter vector     \n",
      "if params are $ \\theta = [ -3,1,1 ] $, then the line for the decision boundary is $ -x + 3 $     "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# notebook magic\n",
      "%matplotlib inline\n",
      "from numpy.random import random, randint\n",
      "\n",
      "def boundary(x):\n",
      "    x = np.float(x)\n",
      "    return (-x + 3)\n",
      "\n",
      "def random_point():\n",
      "    return (1, random()*randint(0,6), random()*randint(0,6))\n",
      "\n",
      "def decide(value):\n",
      "    if value >= 0.5:\n",
      "        return 1\n",
      "    return 0\n",
      "\n",
      "\n",
      "# an example vector of theta values\n",
      "example_theta = np.array([-3,1,1])\n",
      "\n",
      "plt.clf()\n",
      "markers = ['o', '+']\n",
      "colors = ['red', 'blue']\n",
      "points_x = [ random_point() for x in range(100) ]\n",
      "classes = [ decide(sigmoid(p, example_theta)) for p in points_x ]\n",
      "x_1 = [x[1] for x in points_x]\n",
      "x_2 = [x[2] for x in points_x]\n",
      "for i,p in enumerate(points_x):\n",
      "    x_1 = p[1]\n",
      "    x_2 = p[2]\n",
      "    c=classes[i]\n",
      "    plt.scatter(x_1, x_2, s=30.0, c=c, marker=markers[c], color=colors[c])\n",
      "    \n",
      "\n",
      "# map the sigmoid function over a vector of numbers\n",
      "x = np.arange(0, 3, .01)\n",
      "y = [ boundary(n) for n in x ]\n",
      "\n",
      "# add a vertical line at 0\n",
      "#plt.axvline(x=0, ymin=0, ymax=1, ls='--')\n",
      "\n",
      "plt.plot(x, y, color='red', lw=1, ls='--')\n",
      "plt.xlabel('x_1')\n",
      "plt.ylabel('x_2')\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "display_data",
       "png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAEQCAYAAAC5oaP8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
       "text": [
        "<matplotlib.figure.Figure at 0xb174b0c>"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Non-Linear decision boundaries\n",
      "__in logistic regression, to capture a non-linear boundary, we need to add polynomial features__     \n",
      "i.e.    \n",
      "$ h_\\theta(x) = g(\\theta_0 + \\theta_1x_1 + \\theta_2x_2 + \\theta_3x^2_1 + \\theta_4x^2_2) $       \n",
      "       \n",
      "Question: what happens if the $ \\theta $ values for the features above are [-1,0,0,1,1]?      \n",
      "Answer: predict y=1 when $ -1 + x^2_1 + x^2_2 \\geq 0 $       \n",
      "So the decision boundary is: $ x^2_1 + x^2_2 = 1 $ (a circle)               \n",
      "__Remember that the decision boundary is defined by $ \\theta $, not by the dataset__           \n",
      "--> higher-order polynomial features can make arbitrarily complex decision boundaries"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Cost Function -- aka Optimization Objective        \n",
      "We have a training set: $ \\{(x^{(1)}, y^{(1)}), (x^{(2)}, y^{(2)}), ...,(x^{(m)}, y^{(m)}) \\} $          \n",
      "__each $x$ is a vector: $ [x_0, x_1, x_2, ..., x_n] $__         \n",
      "* note that $X_0 = 1$ ($\\theta_0$ is the 'bias' term)        \n",
      "since this is a classification problem, $ y \\in \\{0,1\\} $          \n",
      "       \n",
      "and remember:       \n",
      "$ g(z) = \\frac{1}{1 + e^{-\\theta^Tx}} $          \n",
      "        \n",
      "__So how do we find the right $\\theta$s?__     \n",
      "          \n",
      "For linear regression, the cost function was:         \n",
      "$J(\\theta) = \\frac{1}{m}\\sum\\limits_{i=1}^m\\frac{1}{2}(h_\\theta(x^{(i)}) - y^{(i)})^2 $\n",
      "       \n",
      "Let's represent the part after the sum as:     \n",
      "$ cost(h_\\theta(x^{(i)}, y^{(i)}) $           \n",
      "       \n",
      "So, for linear regression:     \n",
      "$ cost(h_\\theta(x^{(i)}, y^{(i)}) = \\frac{1}{2}(h_\\theta(x^{(i)}) - y^{(i)})^2 $          \n",
      "if we just define $h_\\theta(x)$ as the logistic (sigmoid) function, it's not convex    \n",
      "      \n",
      "So, redefine the cost function as:       \n",
      "          \n",
      "$ cost(h_\\theta(x), y) = \n",
      "\\begin{cases}\n",
      "-log(h_\\theta(x))~~if~~y = 1 \\\\\n",
      "-log(1 - h_\\theta(x))~~if~~y = 0\n",
      "\\end{cases} $                 \n",
      "           \n",
      "      \n",
      "### Get ready, something cool is coming....     \n",
      "because $ y = \\{0, 1\\} $      \n",
      "you can write the cost function succinctly as:      \n",
      "       \n",
      "$ cost(h_\\theta(x^{(i)}, y^{(i)}) = -y~log(h_\\theta(x)) - (1-y)log(1 - h_\\theta(x)) $    \n",
      "      \n",
      "__Important: convince yourself that this is true__       \n",
      "       \n",
      "The final version of the LR cost function for the whole dataset:      \n",
      "$ J(\\theta) = -\\frac{1}{m}[\\sum\\limits_{i=1}^m~y^{(i)}~log(h_\\theta(x^{(i)})) - (1-y^{(i)})log(1 - h_\\theta(x^{(i)}))] $\n",
      "                  "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Training\n",
      "__We want to find:__\n",
      "       \n",
      "$\\displaystyle \\min_{\\theta}J(\\theta) $\n",
      "       \n",
      "given parameters $x$, we predict the $y$ value by:     \n",
      "$ h_\\theta(x) = \\frac{1}{1 + e^{-\\theta^Tx}} $     \n",
      "aka:    \n",
      "$ p(y=1 | x;\\theta) $            \n",
      "       \n",
      "The update template is:      \n",
      "Repeat {       \n",
      "$~~~~\\theta_j := \\theta_j - \\alpha\\frac{\\partial}{\\partial\\theta_j}J(\\theta) $            \n",
      "}     \n",
      "$\\alpha$ is the learning rate     \n",
      "We update all $\\theta_j$ __simultaeneously__     \n",
      "       \n",
      "__Note: don't fear the partial derivative, it's really simple in practice:__     \n",
      "      \n",
      "$ \\frac{\\partial}{\\partial\\theta_j}J(\\theta) = \\frac{1}{m}\\sum\\limits_{i=1}^m(h_\\theta(x^{(i)}) - y^{(i)})x_j^{(i)} $             \n",
      "       \n",
      "__so the update equation becomes: __      \n",
      "\n",
      "Repeat {       \n",
      "$~~~~\\theta_j := \\theta_j - \\alpha\\frac{1}{m}\\sum\\limits_{i=1}^m(h_\\theta(x^{(i)}) - y^{(i)})x_j^{(i)} $            \n",
      "}      \n",
      "__this is identical to the update rule for gradient descent for Linear Regression__      \n",
      "       \n",
      "the vectorized version of the update template (note the missing $j$s):      \n",
      "\n",
      "$~~~~\\theta := \\theta - \\alpha\\frac{1}{m}\\sum\\limits_{i=1}^m[(h_\\theta(x^{(i)}) - y^{(i)})x^{(i)}] $      "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Prepare a Dataset\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# some binary classification datasets can be found here: http://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary.html\n",
      "from sklearn.datasets import load_iris\n",
      "import random\n",
      "\n",
      "# Get some data and munge it into the format we want\n",
      "data = load_iris()\n",
      "binary_iris = np.array(data.target) == 0\n",
      "\n",
      "class_map = { True: 1.0, False: 0.0 }\n",
      "binary_y = np.array([ class_map[c] for c in binary_iris ])\n",
      "\n",
      "# the full datasets\n",
      "X = data.data[:data.data.shape[0] - np.int(data.data.shape[0] / 3), :]\n",
      "y = binary_y[:data.data.shape[0] - np.int(data.data.shape[0] / 3)]\n",
      "\n",
      "# split test/train\n",
      "id_list = range(len(data.target))\n",
      "random.shuffle(id_list)\n",
      "train_X = data.data[[id_list[:120]]]\n",
      "test_X = data.data[[id_list[120:]]]\n",
      "train_y = binary_y[[id_list[:120]]]\n",
      "test_y = binary_y[[id_list[120:]]]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pandas as pd\n",
      "# scikit-learn dimension reduction\n",
      "from sklearn.decomposition import PCA\n",
      "\n",
      "# scikit-learn dataset processing utils\n",
      "from sklearn.preprocessing import MinMaxScaler\n",
      "\n",
      "df = pd.read_csv('./data/train.csv')\n",
      "df = df.astype('float64')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 11,
       "text": [
        "(42000, 785)"
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "min_max_scaler = MinMaxScaler()\n",
      "pca = PCA(n_components=80)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X = min_max_scaler.fit_transform(df.ix[:9999,1:])\n",
      "train_X = pca.fit_transform(X)\n",
      "\n",
      "train_y = df.ix[:9999,0]\n",
      "\n",
      "# Xt = min_max_scaler.transform(df.ix[35000:,1:])\n",
      "test_X = min_max_scaler.transform(df.ix[35000:,1:])\n",
      "test_X = pca.transform(test_X)\n",
      "# yt = df.ix[35000:,0]\n",
      "test_y = df.ix[35000:,0] \n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 21
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class_map = { True: 1.0, False: 0.0 }\n",
      "\n",
      "# binarize train_y and test_y\n",
      "train_y = np.array(train_y) == 1\n",
      "test_y = np.array(test_y) == 1\n",
      "\n",
      "train_y = np.array([ class_map[c] for c in train_y ])\n",
      "test_y = np.array([ class_map[c] for c in test_y ])\n",
      "# test_y[:100]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 22
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 22
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Implement the update equation"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# compute the hypothesis for the row, subtract y, and put the value in every cell of that row\n",
      "# scale each cell by the x_j of that cell (multiply cellwise with the original array)\n",
      "# average columns, and multiply by alpha\n",
      "# subtract that vector from the previous theta to get the new theta for this iteration\n",
      "\n",
      "# this theta will get updated as the algorithm iterates\n",
      "# theta = np.random.uniform(-1.0,1.0,[1,X.shape[1]])\n",
      "theta = np.zeros((1,train_X.shape[1]))\n",
      "original_theta = np.array(theta)\n",
      "lambd = 1.0\n",
      "\n",
      "# completely vectorized implementation\n",
      "# NOTE: not regularized -- do this as an exercise\n",
      "def iterate(X, y, theta, alpha):\n",
      "    updates = np.average(X * (sigmoid(X, theta).T - y).T, axis=0) * alpha \n",
      "    new_theta = theta - updates\n",
      "    return new_theta\n",
      "\n",
      "# just for demo purposes - a more straightforward version of the function above\n",
      "# regularized iterate\n",
      "def regularized_simple_iterate(X, y, theta, alpha):\n",
      "    hypotheses = sigmoid(X, theta)\n",
      "    minus_y = hypotheses.T - y\n",
      "    scaled_by_x = X * minus_y.T\n",
      "    col_averages = np.average(scaled_by_x, axis=0)\n",
      "    updates = col_averages* alpha\n",
      "    \n",
      "    updates2 = col_averages*alpha -lambd*theta[0]*alpha\n",
      "    updates2[0] = updates2[0]+ (lambd*theta[0][0]*alpha)\n",
      "    new_theta = theta - updates\n",
      "    regularized_theta = theta - updates2\n",
      "   \n",
      "    return regularized_theta\n",
      "\n",
      "def simple_iterate(X, y, theta, alpha):\n",
      "    hypotheses = sigmoid(X, theta)\n",
      "    minus_y = hypotheses.T - y\n",
      "    scaled_by_x = X * minus_y.T\n",
      "    col_averages = np.average(scaled_by_x, axis=0)\n",
      "    updates = col_averages* alpha\n",
      "    new_theta = theta - updates\n",
      "    return new_theta"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 38
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Evaluate performance"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# let's test the accuracy of our classifier\n",
      "def predict(x, theta):\n",
      "    return sigmoid(x, theta)\n",
      "\n",
      "def classes(y):\n",
      "    if y == 1.0:\n",
      "        return True\n",
      "    return False\n",
      "\n",
      "def check_accuracy(predictions, y):\n",
      "    binarized_predictions = predictions >= 0.5\n",
      "    tf = np.array([ classes(c) for c in y ], dtype='bool')\n",
      "    correct = binarized_predictions[:,0] == tf\n",
      "    score = np.sum([ 1 for s in correct if s == True ]) / np.float(len(correct))\n",
      "    return(score)\n",
      "  "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 39
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# To predict, all we need is a theta - let's check performance before training\n",
      "check_accuracy(predict(test_X, original_theta), test_y)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 40,
       "text": [
        "0.11157142857142857"
       ]
      }
     ],
     "prompt_number": 40
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Regularized Cost Function"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# note: this is just for sanity checks\n",
      "def cost_function(X, answers, theta):\n",
      "    total = 0.0\n",
      "    for x,y in zip(X,answers):\n",
      "        hyp = sigmoid(x, theta)\n",
      "        total += ( (-y * np.log(hyp)) - ((1-y) * np.log(1-hyp)) ) \n",
      "    cost = (total / len(answers))\n",
      "    return cost"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 42
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# note: this is just for sanity checks\n",
      "def regularized_cost_function(X, answers, theta):\n",
      "    total = 0.0\n",
      "    for x,y in zip(X,answers):\n",
      "        hyp = sigmoid(x, theta)\n",
      "        total += ( (-y * np.log(hyp)) - ((1-y) * np.log(1-hyp)) ) \n",
      "    cost = (total / len(answers)) + ((lambd/len(answers))*sum(theta[1:]*theta[1:]))\n",
      "    return cost"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 41
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# print the original theta, and the output of the cost function\n",
      "print(theta)\n",
      "print(cost_function(train_X, train_y, theta))\n",
      "test =  []\n",
      "\n",
      "def gradient_descent(X, y, theta, alpha, num_iterations=100):\n",
      "    working_theta = np.array(theta)\n",
      "    all_cost =[]\n",
      "    for i in range(num_iterations):\n",
      "#         working_theta = regularized_simple_iterate(X, y, working_theta, alpha)\n",
      "        working_theta = simple_iterate(X, y, working_theta, alpha)\n",
      "#         all_cost.append(regularized_cost_function(train_X, train_y, working_theta))\n",
      "        all_cost.append(cost_function(train_X, train_y, working_theta))\n",
      "        #j = j+1\n",
      "    test.append(all_cost)    \n",
      "    return np.array(working_theta)\n",
      "        \n",
      "num_iterations = 30\n",
      "j = 0   \n",
      "\n",
      "alpha = 0.005\n",
      "\n",
      "best_theta = gradient_descent(train_X, train_y, theta, alpha, num_iterations)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
        "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
        "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
        "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
        "   0.  0.  0.  0.  0.  0.  0.  0.]]\n",
        "[ 0.69314718]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 48
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "predictions = predict(test_X, best_theta)    \n",
      "check_accuracy(predictions, test_y)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 49,
       "text": [
        "0.66400000000000003"
       ]
      }
     ],
     "prompt_number": 49
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# test with some different learning rates\n",
      "alpha = [0.005,0.01,0.03,0.06,0.1]\n",
      "for i in alpha:\n",
      "    best_theta = gradient_descent(train_X, train_y, theta, i, num_iterations)\n",
      "\n",
      "    j=j+1\n",
      "    print i;\n",
      "all_cost\n",
      "\n",
      "plt.clf()\n",
      "plt.plot(test[0],color='yellow',lw=2)\n",
      "plt.plot(test[1],color='blue', lw=2)\n",
      "plt.plot(test[2],color='green', lw=2)\n",
      "plt.plot(test[3],color='red', lw=2)\n",
      "plt.plot(test[4],color='grey', lw=2)\n",
      "plt.xlabel('hypothesis')\n",
      "plt.ylabel('cost')\n",
      "plt.ylim([0,0.2])\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# the following graph shows what the cost function looks like for y = 1 and y = 0\n",
      "plt.clf()\n",
      "\n",
      "def positive_cost(hyp_value):\n",
      "    return -(np.log(hyp_value))\n",
      "\n",
      "def negative_cost(hyp_value):\n",
      "    return -(np.log(1 - hyp_value))\n",
      "\n",
      "# map cost function over some numbers\n",
      "pos1_x = np.arange(0.01, 1, .01)\n",
      "pos1_y = [ positive_cost(n) for n in pos1_x ]\n",
      "\n",
      "\n",
      "neg1_x = np.arange(0.01, 1, .01)\n",
      "neg1_y = [ negative_cost(n) for n in neg1_x ]\n",
      "\n",
      "# add a horizontal line at 0\n",
      "#plt.axvline(x=0, ymin=0, ymax=1, ls='--')\n",
      "\n",
      "plt.plot(pos1_x, pos1_y, color='blue', lw=2)\n",
      "plt.plot(neg1_x, neg1_y, color='red', lw=2)\n",
      "\n",
      "plt.xlabel('hypothesis')\n",
      "plt.ylabel('cost')\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "__Question: Explain why this is a good cost function for learning the parameters of a binary classifier__"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Exercises:      \n",
      "(1) Plot the cost function as logistic regression iterates      \n",
      "(2) Try several different learning rates (alphas), and plot what happens when alpha changes (your plot should have several lines)     \n",
      "(3) Load a new dataset and try Logistic Regression on that dataset     "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 12
    }
   ],
   "metadata": {}
  }
 ]
}