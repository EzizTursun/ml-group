{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# FEEDFORWARD NEURAL NETWORKS"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "- this implementation is an extention of this great blog post: http://rolisz.ro/2013/04/18/neural-networks-in-python/"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "\n",
      "def tanh(x):\n",
      "    return np.tanh(x)\n",
      "\n",
      "def tanh_deriv(x):\n",
      "    return 1.0 - x**2\n",
      "\n",
      "def logistic(x):\n",
      "    return 1/(1 + np.exp(-x))\n",
      "\n",
      "def logistic_derivative(x):\n",
      "    return logistic(x)*(1-logistic(x))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 156
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class NeuralNetwork:\n",
      "\n",
      "    def __init__(self, layers, activation='tanh'):\n",
      "         \"\"\"\n",
      "         :param layers: A list containing the number of units in each layer. Should be at least two values\n",
      "         :param activation: The activation function to be used. Can be \"logistic\" or \"tanh\"\n",
      "         -\"\"\"\n",
      "         if activation == 'logistic':\n",
      "               self.activation = logistic\n",
      "               self.activation_deriv = logistic_derivative\n",
      "         elif activation == 'tanh':\n",
      "               self.activation = tanh\n",
      "               self.activation_deriv = tanh_deriv\n",
      "\n",
      "         # initialize the weights randomly\n",
      "         self.weights = []\n",
      "         for i in range(1, len(layers) - 1):\n",
      "               self.weights.append((2*np.random.random((layers[i - 1] + 1, layers[i] + 1))-1)*0.25)\n",
      "         # initialize the weights between the last hidden layer and the output layer \n",
      "         # note that we don't add 1 to the last dimension\n",
      "         self.weights.append((2*np.random.random((layers[i] + 1, layers[i + 1]))-1)*0.25)\n",
      "         \n",
      "         \n",
      "    def fit(self, X, y, learning_rate=0.2, epochs=10000):\n",
      "         X = np.atleast_2d(X)\n",
      "                   \n",
      "         # add the bias units as the last column of X\n",
      "         bias_column = np.ones((X.shape[0],1))\n",
      "         X = np.hstack((X, bias_column))\n",
      "         y = np.array(y)\n",
      "\n",
      "         # ok let's train this mofo with some SGD\n",
      "         for k in range(epochs):\n",
      "               # select a random training instance\n",
      "               i = np.random.randint(X.shape[0])\n",
      "               a = [X[i]]\n",
      "\n",
      "               # go through the layers, and compute the activations of each node\n",
      "               for l in range(len(self.weights)):\n",
      "                         a.append(self.activation(np.dot(a[l], self.weights[l])))\n",
      "               # compute the classification error\n",
      "               error = y[i] - a[-1]\n",
      "               # the errors between the output layer and l-1\n",
      "               deltas = [error * self.activation_deriv(a[-1])]\n",
      "\n",
      "               # now we're going backwards\n",
      "               for l in range(len(a) - 2, 0, -1): # we need to begin at the second to last layer\n",
      "                    deltas.append(deltas[-1].dot(self.weights[l].T)*self.activation_deriv(a[l]))\n",
      "               # we were appending, but actually moving backwards through the network, so we need to flip the deltas around\n",
      "               deltas.reverse()\n",
      "               for i in range(len(self.weights)):\n",
      "                    layer = np.atleast_2d(a[i])\n",
      "                    delta = np.atleast_2d(deltas[i])\n",
      "                    self.weights[i] += learning_rate * layer.T.dot(delta)\n",
      "                    \n",
      "    def predict(self, x):\n",
      "         x = np.array(x)\n",
      "         # add the bias unit to the input features\n",
      "         x = np.hstack((x, np.ones(1)))\n",
      "         a = x\n",
      "         for l in range(0, len(self.weights)):\n",
      "               a = self.activation(np.dot(a, self.weights[l]))\n",
      "         return a\n",
      "    \n",
      "    def output_activations(self, x):\n",
      "         x = np.array(x)\n",
      "         # add the bias unit to the input features\n",
      "         x = np.hstack((x, np.ones(1)))\n",
      "         activations = []\n",
      "         activations.append(x)\n",
      "         a = x\n",
      "         for l in range(0, len(self.weights)):\n",
      "              a = self.activation(np.dot(a, self.weights[l]))\n",
      "              activations.append(a)\n",
      "         return activations"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 157
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Let's learn a XOR classifier (the classic example)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "nn = NeuralNetwork([2,2,1], 'tanh')\n",
      "X = np.array([[0, 0],\n",
      "              [0, 1],\n",
      "              [1, 0],\n",
      "              [1, 1]])\n",
      "y = np.array([0, 1, 1, 0])\n",
      "nn.fit(X, y)\n",
      "for i in [[0, 0], [0, 1], [1, 0], [1,1]]:\n",
      "    print(i,nn.predict(i))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "([0, 0], array([  9.53745708e-05]))\n",
        "([0, 1], array([ 0.98738091]))\n",
        "([1, 0], array([ 0.98747907]))\n",
        "([1, 1], array([ 0.00020526]))\n"
       ]
      }
     ],
     "prompt_number": 158
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Cool, let's test the implementation on the famous digits dataset"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "from sklearn.datasets import load_digits\n",
      "from sklearn.metrics import confusion_matrix, classification_report\n",
      "from sklearn.preprocessing import LabelBinarizer\n",
      "from sklearn.cross_validation import train_test_split\n",
      "\n",
      "digits = load_digits()\n",
      "X = digits.data\n",
      "y = digits.target\n",
      "X -= X.min()     # normalize the values to bring them into the range 0-1\n",
      "X /= X.max()\n",
      "\n",
      "# nn = NeuralNetwork([64,100,10],'tanh')\n",
      "nn = NeuralNetwork([64, 100, 100, 10],'logistic')\n",
      "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
      "labels_train = LabelBinarizer().fit_transform(y_train)\n",
      "labels_test = LabelBinarizer().fit_transform(y_test)\n",
      "\n",
      "nn.fit(X_train,labels_train,epochs=30000)\n",
      "predictions = []\n",
      "for i in range(X_test.shape[0]):\n",
      "    o = nn.predict(X_test[i] )\n",
      "    predictions.append(np.argmax(o))\n",
      "print confusion_matrix(y_test,predictions)\n",
      "print classification_report(y_test,predictions)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[[35  0  0  0  1  0  0  0  0  0]\n",
        " [ 1 52  0  0  0  0  0  0  0  0]\n",
        " [ 0  0 48  0  0  0  0  0  0  0]\n",
        " [ 0  0  0 39  0  0  0  0  0  1]\n",
        " [ 0  0  0  0 38  0  0  0  0  4]\n",
        " [ 0  0  0  0  1 52  0  0  0  2]\n",
        " [ 0  0  0  0  0  0 45  0  1  0]\n",
        " [ 0  0  0  0  1  0  0 43  0  1]\n",
        " [ 0  2  0  0  0  0  0  0 41  1]\n",
        " [ 0  1  0  0  0  0  0  0  1 39]]\n",
        "             precision    recall  f1-score   support\n",
        "\n",
        "          0       0.97      0.97      0.97        36\n",
        "          1       0.95      0.98      0.96        53\n",
        "          2       1.00      1.00      1.00        48\n",
        "          3       1.00      0.97      0.99        40\n",
        "          4       0.93      0.90      0.92        42\n",
        "          5       1.00      0.95      0.97        55\n",
        "          6       1.00      0.98      0.99        46\n",
        "          7       1.00      0.96      0.98        45\n",
        "          8       0.95      0.93      0.94        44\n",
        "          9       0.81      0.95      0.88        41\n",
        "\n",
        "avg / total       0.96      0.96      0.96       450\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 159
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%matplotlib inline\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "X = digits.data[60]\n",
      "X -= X.min()     # normalize the values to bring them into the range 0-1\n",
      "X /= X.max()\n",
      "data = X.reshape((8,8))\n",
      "\n",
      "fig, ax = plt.subplots()\n",
      "heatmap = ax.pcolor(data, cmap=plt.cm.Oranges)\n",
      "\n",
      "ax.invert_yaxis()\n",
      "ax.xaxis.tick_top()\n",
      "ax.set_aspect('equal')\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "display_data",
       "png": "iVBORw0KGgoAAAANSUhEUgAAAPkAAAEACAYAAABxpdD1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nkXCMnEg4Rk4kHCMnEo6REwnHyImE+3/NQ1QyAgFQywAAAABJRU5ErkJggg==\n",
       "text": [
        "<matplotlib.figure.Figure at 0x5a11950>"
       ]
      }
     ],
     "prompt_number": "*"
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## let's vizualize the activations that we learned!"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# NOTE: this is a very inefficient method!\n",
      "all_activations = []\n",
      "layers = [64, 100, 100, 10]\n",
      "for layer_idx in range(1, len(layers)):\n",
      "    layer_activations = []\n",
      "    for node_idx in range(layers[layer_idx]):\n",
      "        training_activations = np.array([nn.output_activations(x)[layer_idx][node_idx] for x in X_train])\n",
      "        weighted_instances = np.array([training_activations[i] * instance for i,instance in enumerate(X_train)])\n",
      "        # get the activation of this node for each instance\n",
      "        # weight each instance by the node's activation\n",
      "        # average, then plot\n",
      "        avg_weights = np.average(weighted_instances, axis=0)\n",
      "        layer_activations.append(avg_weights.reshape((8,8)))\n",
      "    all_activations.append(layer_activations)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": "*"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pylab, matplotlib\n",
      "plt.close()\n",
      "\n",
      "# TODO: set the name of the layer we're plotting\n",
      "\n",
      "# plot the inner layers\n",
      "for layer_idx in range(len(all_activations)-1):\n",
      "    fig = plt.figure()\n",
      "    fig = matplotlib.pyplot.gcf()\n",
      "    fig.set_size_inches(18.5,10.5)\n",
      "\n",
      "    for idx,res in enumerate(all_activations[layer_idx]):\n",
      "        ax = fig.add_subplot(10,10,idx)\n",
      "        heatmap = ax.pcolor(res, cmap=plt.cm.Oranges)\n",
      "        ax.xaxis.set_major_locator(pylab.NullLocator())\n",
      "        ax.yaxis.set_major_locator(pylab.NullLocator())\n",
      "    \n",
      "        ax.invert_yaxis()\n",
      "        ax.set_aspect('equal')\n",
      "\n",
      "# plot the last layer\n",
      "fig = plt.figure()\n",
      "fig = matplotlib.pyplot.gcf()\n",
      "\n",
      "for idx,res in enumerate(all_activations[-1]):\n",
      "    ax = fig.add_subplot(1,10,idx)\n",
      "    heatmap = ax.pcolor(res, cmap=plt.cm.Oranges)\n",
      "    ax.xaxis.set_major_locator(pylab.NullLocator())\n",
      "    ax.yaxis.set_major_locator(pylab.NullLocator())\n",
      "\n",
      "    ax.invert_yaxis()\n",
      "    ax.set_aspect('equal')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": "*"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "data = avg_weights.reshape((8,8))\n",
      "fig, ax = plt.subplots()\n",
      "\n",
      "heatmap = ax.pcolor(data, cmap=plt.cm.Oranges)\n",
      "\n",
      "# want a more natural, table-like display\n",
      "ax.invert_yaxis()\n",
      "ax.xaxis.tick_top()\n",
      "ax.set_aspect('equal')\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "display_data",
       "png": "iVBORw0KGgoAAAANSUhEUgAAAPkAAAEACAYAAABxpdD1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
       "text": [
        "<matplotlib.figure.Figure at 0x587ef50>"
       ]
      }
     ],
     "prompt_number": 113
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Questions:\n",
      "# what is the practical difference between the 'logistic' and 'tanh' activation functions?\n",
      "# what happens when you change the architecture of the network?\n",
      "# what examples does the network get wrong? -- plot them and try to understand why"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}