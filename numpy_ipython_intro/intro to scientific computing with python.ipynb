{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# prevent confusing errors\n",
      "from __future__ import print_function, division\n",
      "import pprint, os\n",
      "pp = pprint.PrettyPrinter(indent=4)\n",
      "\n",
      "# todo: also add 2to3 here?"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 31
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n",
      "from [the Scikit-learn docs](http://scikit-learn.org/stable/datasets/#downloading-datasets-from-the-mldata-org-repository)\n",
      "\n",
      "The sklearn.datasets.fetch_20newsgroups function is a data fetching / caching functions that downloads the data archive from the original 20 newsgroups website, extracts the archive contents in the ~/scikit_learn_data/20news_home folder and calls the sklearn.datasets.load_file on either the training or testing set folder, or both of them"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# importing datasets (lots of sample data here: http://mldata.org/)\n",
      "# note: pandas can be really helpful for this\n",
      "import numpy as np\n",
      "import scipy as sp\n",
      "import sklearn\n",
      "\n",
      "# define a custom home for our data (just lets us keep the data on github, instead of downloading it again)\n",
      "custom_data_home = str(os.path.join(os.getcwd(), 'data'))\n",
      "newsgroups = sklearn.datasets.fetch_20newsgroups(subset='all', data_home=custom_data_home)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 37
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# investigate the structure of this data\n",
      "pp.pprint(newsgroups.keys())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "['DESCR', 'data', 'target', 'target_names', 'filenames']\n"
       ]
      }
     ],
     "prompt_number": 38
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pp.pprint(newsgroups.target_names)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[   'alt.atheism',\n",
        "    'comp.graphics',\n",
        "    'comp.os.ms-windows.misc',\n",
        "    'comp.sys.ibm.pc.hardware',\n",
        "    'comp.sys.mac.hardware',\n",
        "    'comp.windows.x',\n",
        "    'misc.forsale',\n",
        "    'rec.autos',\n",
        "    'rec.motorcycles',\n",
        "    'rec.sport.baseball',\n",
        "    'rec.sport.hockey',\n",
        "    'sci.crypt',\n",
        "    'sci.electronics',\n",
        "    'sci.med',\n",
        "    'sci.space',\n",
        "    'soc.religion.christian',\n",
        "    'talk.politics.guns',\n",
        "    'talk.politics.mideast',\n",
        "    'talk.politics.misc',\n",
        "    'talk.religion.misc']\n"
       ]
      }
     ],
     "prompt_number": 39
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# ok, let's randomly select 5 of those categories\n",
      "categories = list(np.random.choice(newsgroups.target_names, 5))\n",
      "categories"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 40,
       "text": [
        "['sci.space', 'comp.graphics', 'rec.sport.baseball', 'sci.med', 'sci.med']"
       ]
      }
     ],
     "prompt_number": 40
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# now let's deal only with those categories\n",
      "newsgroups_pruned = sklearn.datasets.fetch_20newsgroups(remove=('headers', 'footers', 'quotes'),\n",
      "                                     categories=categories)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 41
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# sanity check\n",
      "assert(newsgroups_pruned.filenames.shape == newsgroups_pruned.target.shape)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 42
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# let's take a look at the distribution of the categories in our data"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 20,
       "text": [
        "11314"
       ]
      }
     ],
     "prompt_number": 20
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# visualizing your data"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# creating slides from your ipython notebook"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Notes and gotchas"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "see [Classification Pipelines](http://scikit-learn.org/stable/auto_examples/grid_search_text_feature_extraction.html#example-grid-search-text-feature-extraction-py) for a more advanced example"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}